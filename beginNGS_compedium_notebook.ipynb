{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["\"\"\"\n","This generates a report of variants matching Clinvar variants in 384 genes with zygosity and compound hets annotated\n","\"\"\"\n","import tiledb.cloud\n","import math\n","import pandas as pd\n","import numpy\n","import tiledb\n","from tiledb.cloud.dag import dag\n","import os\n","from datetime import datetime\n","from cryptography.fernet import Fernet\n","import pyarrow\n","from typing import Iterable, List, Optional\n","import logging\n","logger = logging.getLogger(__name__)\n","\n","pd.set_option('display.max_rows', None)\n","\n","key = Fernet.generate_key() #this is your \"password\"\n","cipher_suite = Fernet(key)\n","\n","#TileDB-VCF array to use\n","TILEDB_VCF_URI = {}\n","TILEDB_VCF_URI['RADYPATIENTS'] = \"tiledb://Rady_Childrens_Institute_Genomic_Medicine/####-####-####-####-####\"\n","TILEDB_VCF_URI['PRELIMINARY-PHASE-2-PROSPECTIVE-SAMPLES-RE-PROCESSING'] = \"tiledb://Rady_Childrens_Institute_Genomic_Medicine/####-####-####-####-####\"\n","VEP_URI = \"tiledb://Rady_Childrens_Institute_Genomic_Medicine/vep-variants-dragen-3-9-3-hg38-graph-based-vcfs\"\n","SAMPLE_METADATA_URI = 'tiledb://Rady_Childrens_Institute_Genomic_Medicine/####-####-####-####-####'\n","\n","GNOMAD_3_1_URI = \"tiledb://Rady_Childrens_Institute_Genomic_Medicine/gnomad-annotations\"\n","GNOMAD_4_0_URI = \"tiledb://TileDB-Inc/gnomad-4_0-include-nopass\"\n","GNOMAD_URI = GNOMAD_3_1_URI\n","\n","#use this variant file\n","VARIANT_SELECTION = 'ANNOTATED_VARIANT_DB'\n","#VCF selection\n","VCF_SELECTION = 'RADYPATIENTS'\n","\n","#the csv variants of interest file in tiledb\n","VARIANT_FILE_URI = {}\n","\n","VARIANT_FILE_URI['ANNOTATED_VARIANT_DB'] = \"../../data/joint_variants.csv\"\n","VARIANT_FILE = VARIANT_FILE_URI[VARIANT_SELECTION] \n","\n","#where the filedb array that holds the variant list is stored\n","VARIANT_URI_BASE_NAME = {}\n","\n","# update this if the variant list OR THE BLOCKLIST is altered\n","VARIANT_URI_BASE_NAME['ANNOTATED_VARIANT_DB'] = \"ANNOTATED_VARIANT_DB\"         #\n","VARIANT_URI = 'tiledb://Rady_Childrens_Institute_Genomic_Medicine/s3://tiledb-rchsd-arrays/'+VARIANT_URI_BASE_NAME[VARIANT_SELECTION]\n","\n","#where the report is stored\n","adjust_position_for_clinvar = False\n","\n","#GQ/DP filters\n","FILTER_FOR_QUALITY = False\n","\n","FINAL_REPORT_BASE_NAME = {}\n","\n","FINAL_REPORT_BASE_NAME['ANNOTATED_VARIANT_DB'] = \"ANNOTATED_VARIANT_DBfilteredreport\"\n","\n","import datetime\n","current_datetime = datetime.datetime.now()\n","formatted_datetime = current_datetime.strftime(\"%Y%m%d%H%M%S\")\n","REPORT_URI = 'tiledb://Rady_Childrens_Institute_Genomic_Medicine/s3://tiledb-rchsd-arrays/production/'+FINAL_REPORT_BASE_NAME[VARIANT_SELECTION]+\"_\"+formatted_datetime\n","\n","#patterns of inheritance\n","NBS_POI_FILE = \"../../data/phase2_beginngs_moi_03292024.txt\"\n","\n","BLOCKLIST_FILE = \"../../data/prelim_blocklist_03292024_dups_removed.csv\"\n","\n","USE_BLOCKLIST = False\n","\n","SAMPLE_USE_FILE = '../../data/rady_sample_approved_addl_use_indicator_19JUN2023.csv'\n","\n","today_date = datetime.datetime.now().strftime(\"%d%m%Y\")\n","namespace = \"Rady_Childrens_Institute_Genomic_Medicine\"\n","\n","#convert to standard hugo gene names\n","convert_to_hugo = False\n","debug = False\n","\n","#turn off future warnings\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#create a dataframe with name, description, stage, unit, value\n","metric = pd.DataFrame(columns=['name','description','stage','unit','value'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["DAG_MODE = tiledb.cloud.dag.Mode.REALTIME\n","\n","#these are only useful for batch mode\n","DEFAULT_RESOURCES = {\"cpu\": \"8\", \"memory\": \"4Gi\"}\n","DEFAULT_IMG_NAME = \"3.9-genomics\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def query_sample_metadata(\n","    sample_stems: list,\n","    family_roles: list,\n","    genome_id_list: pyarrow.lib.StringArray,\n","    remove_sample_name_suffix: bool = True,\n",") -> pyarrow.Table:\n","    \"\"\"\n","    This function queries the metadata to lookup sample names from genome ids\n","    Arguments:\n","        @param genome_id_list: the genome_ids of interest\n","    Returns:\n","        Sample data subset\n","    \"\"\"\n","    import pandas\n","    import pyarrow\n","    import tiledb\n","\n","    sample_metadata_attrs = [\n","        \"site_id\",\n","        \"study_id\",\n","        \"case_id\",\n","        \"individual_id\",\n","        \"biospecimen\",\n","        \"aliquot_id\",\n","        \"test_type\",\n","        \"family_relationship\",\n","        \"version\",\n","        \"sample_name\",\n","        \"gender\",\n","        \"fabric_report_id\",\n","    ]\n","    with tiledb.open(SAMPLE_METADATA_URI) as A:\n","        df = A.query(attrs=sample_metadata_attrs)[:]\n","        df = pandas.DataFrame(df)\n","        if sample_stems is not None and len(sample_stems) > 0:\n","            df = df[df.sample_name.str.contains(\"|\".join(sample_stems))]\n","        if family_roles is not None and len(family_roles) > 0:\n","            df = df[df.family_relationship.isin(family_roles)]\n","        if genome_id_list is not None and len(genome_id_list) > 0:\n","            genome_ids = genome_id_list.to_pylist()\n","            df = df[df.genome_id.isin(genome_ids)]\n","        #remove -1 from end of sample names\n","        if remove_sample_name_suffix:\n","            df[\"sample_name\"] = df[\"sample_name\"].str.replace(\"-1$\", \"\")#, regex=True)\n","    return pyarrow.Table.from_pandas(df)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This fetches the subset of samples we are interested in. It also fetches sample metadata.\n","Because the sample metadata has been comprimised we are not going to use this at the moment as a sample subset."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample_graph = dag.DAG(max_workers=1, namespace=namespace, name = \"fetch samples nbs bill\")\n","filtered_metadata_node = sample_graph.submit(\n","    query_sample_metadata,\n","    sample_stems=None,\n","    family_roles=None,\n","    genome_id_list=None,\n","    remove_sample_name_suffix=True,\n","    name=f\"Sample Metadata Filter for Probands\",\n","    resource_class=\"standard\",\n","    \n",")\n","sample_graph.compute()\n","sample_graph.wait()\n","filtered_metadata_pre_use = filtered_metadata_node.result().to_pandas()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#metric = pd.DataFrame(columns=['name','description','stage','unit','value'])\n","#add the filtered metadata count\n","metric = metric.append({'name':'filtered_metadata_count','description':'Number of samples in filtered metadata','stage':'pre-use','unit':'count','value':filtered_metadata_pre_use.shape[0]},ignore_index=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Merge with usage indicator file"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_use = pd.read_csv(SAMPLE_USE_FILE)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["filtered_metadata_use = filtered_metadata_pre_use.merge(sample_use, how='inner', left_on=['individual_id','family_relationship','case_id'], right_on=['ind_id','family_relationship','case_id'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["metric = metric.append({'name':'filtered_metadata_count','description':'Number of samples in filtered metadata','stage':'post-sample-use','unit':'count','value':filtered_metadata_use.shape[0]},ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["filtered_metadata = filtered_metadata_use[filtered_metadata_use['additional_use'] == 'Yes']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["metric = metric.append({'name':'filtered_metadata_count','description':'Number of samples in filtered metadata','stage':'additional_use is Yes','unit':'count','value':filtered_metadata.shape[0]},ignore_index=True)"]},{"cell_type":"raw","metadata":{},"source":["Let's make sure there are no -1's in these sample names "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sm_samples = list(filtered_metadata['sample_name'])\n","len(sm_samples)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if not os.path.isfile(NBS_POI_FILE):\n","    tiledb.cloud.file.export_file(\"tiledb://Rady_Childrens_Institute_Genomic_Medicine/10fcd805-7a4b-4ed6-8486-f46299cade58\",output_uri=NBS_POI_FILE)\n","else:\n","    print(f\"NBS_POI_FILE already exists {NBS_POI_FILE}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This read_partition is meant to be run on a gene basis at the lowest granularity, so it can find compound hets"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def read_partition(\n","        uri: str,\n","        regions_by_gene: List[List[str]],\n","        sample_partition_count: int,\n","        sample_partition_id: int,\n","        *,\n","        samples: Optional[List[str]] = None,\n","        region_partition_count: Optional[int] = None,\n","        selected_variants_uri: Optional[str] = None,\n","        region_partition_id: Optional[int] = None,\n","        multiallelic_treatment: str = \"decompose\",\n","        drop_duplicates: bool = True,\n","        remove_sample_name_suffix=True,\n","        subtract_start_pos_by_one_for_deletions: bool = False,\n","        variant_selection = None,\n","        filter_for_quality = False,\n","        **kwargs\n",") -> pyarrow.Table:\n","    \"\"\"Reads a single partition's worth of VCF data.\n","\n","    multiallelic_treatment: \"hide\" or \"decompose\"\n","    \"\"\"\n","    import pprint\n","    import tiledbvcf\n","    import pandas as pd\n","    import pyarrow as pa\n","    import re\n","\n","\n","    cfg = tiledbvcf.ReadConfig(\n","        sample_partition=(sample_partition_id, sample_partition_count)\n","    )\n","\n","            \n","    # Printed output will be available in the task logs within TileDB cloud\n","    # once the task is complete. (It will not be seen locally.)\n","    print(\"loading regions:\")\n","    pprint.pprint(f\"{region_partition_id} of {region_partition_count}\")\n","    print(\"with configuration:\")\n","    pprint.pprint(cfg)\n","\n","    vcf_ds = tiledbvcf.Dataset(uri, mode=\"r\", stats=True, cfg=cfg)\n","\n","    #attrs = vcf_ds.attributes()\n","    attrs = [\"sample_name\", \"contig\", \"pos_start\", \"pos_end\", \"alleles\", \"fmt_GT\", \"fmt_DP\",\"fmt_GQ\"]\n","    if multiallelic_treatment == 'hide':\n","        attrs += [\"info_AN\"]\n","    #this is mostly boilerplate we use for validating input ranges\n","    clean_regions = []\n","    contigs = []\n","    \n","    #remember regions_by_gene is a list snp by a list of genes\n","    #if you really have more than one region do the gene, else do all genes\n","    if region_partition_count is not None and region_partition_count>1:\n","        \n","        # break the regions into partitions\n","        regions_by_partitions = numpy.array_split(regions_by_gene, region_partition_count)\n","        region_in_partition = regions_by_partitions[region_partition_id]\n","        regions = [snp for gene in region_in_partition for snp in gene]\n","    else:\n","        regions = [snp for gene in regions_by_gene for snp in gene]\n","\n","    for region in regions:\n","        regexgroups = re.match(\"(chr[0-9XYMT]+):([0-9]+)(-([0-9]+))?\", region.replace(\" \", \"\"))\n","        if regexgroups is None:\n","            print(\"Invalid genomic coordinate: {genomic_coodinate}\")\n","            return None\n","        regexchr = regexgroups.group(1)\n","        regexstart = int(regexgroups.group(2))\n","        if regexgroups.group(3) is None:\n","            regexend = regexstart  # point coordinate\n","        else:\n","            regexend = int(regexgroups.group(4))  # lose the dash\n","        contigs += [regexchr]\n","        clean_regions += [f\"{regexchr}:{regexstart}-{regexend}\"]\n","    \n","    clean_string = ','.join(clean_regions)[0:15]\n","    print(f\"loading {clean_string}...{len(clean_regions)} regions\")\n","    vcf_dfs = []\n","    vcf_dfs.append(vcf_ds.read_arrow(\n","        attrs=attrs,\n","        regions=clean_regions,\n","        samples=samples\n","    ))\n","    while not vcf_ds.read_completed():\n","        print(\"still more to go...\")\n","        vcf_dfs.append(vcf_ds.continue_read_arrow())\n","    \n","    vcf_df = pyarrow.concat_tables(vcf_dfs).to_pandas()\n","    \n","\n","    if vcf_df is None or len(vcf_df) == 0:\n","        return pyarrow.Table.from_pandas(pandas.DataFrame())\n","    else:\n","        print(f\"{len(vcf_df)} results in vcf\")\n","    \n","    alleles = pd.DataFrame(vcf_df[\"alleles\"].to_list())\n","    vcf_df[\"ref\"] = alleles[0]\n","    vcf_df[\"alt\"] = alleles.drop([0], axis=1).values.tolist()\n","    \n","    if remove_sample_name_suffix == True:\n","        vcf_df[\"sample_name\"] = vcf_df[\"sample_name\"].str.replace(\"-1$\", \"\") \n","    \n","    #filter for depth and genotype quality\n","    if filter_for_quality == True:\n","        vcf_df = vcf_df[(vcf_df['fmt_GQ'] > 20) & (vcf_df['fmt_DP'] > 8)]\n","    \n","    if multiallelic_treatment == 'hide':\n","        vcf_df = vcf_df.loc[vcf_df['info_AN'] < 3]\n","    \n","    # multiple alleles get their own rows\n","    if multiallelic_treatment == 'decompose':\n","        vcf_df = vcf_df.explode(\"alt\")\n","        vcf_df = vcf_df[vcf_df[\"alt\"].notnull()]\n","\n","    if drop_duplicates:\n","        #make these \"hashable\" for downstream operations\n","        vcf_df[\"alleles\"] = vcf_df[\"alleles\"].apply(lambda x: tuple(x))\n","        vcf_df[\"fmt_GT\"] = vcf_df[\"fmt_GT\"].apply(lambda x: tuple(x))\n","        vcf_df = vcf_df.drop_duplicates()\n","        print(f\"{len(vcf_df)} results in vcf after dedup\")\n","\n","    def classifyZygosity(x):\n","        #rady did not ingest gVCFs so HOM_REF is not possible\n","        if tuple(x) == tuple([0, 0]):\n","            raise Exception(\"HOM_REF not possible\")\n","        elif tuple(x) == tuple([0, 1]):\n","            return \"HET\"\n","        elif tuple(x) == tuple([1, 1]):\n","            return \"HOM_ALT\"\n","        elif tuple(x) == tuple([1]):\n","            return \"HEMI\"\n","        else:\n","            if multiallelic_treatment == 'hide':\n","                raise Exception(f\"UNKNOWN zygosity {x}\")\n","            else:\n","                if x[0]==x[1]:\n","                    return \"HOM_ALT\"\n","                else:\n","                    #look for other edge cases\n","                    return \"HET\"\n","    vcf_df['zygosity'] = vcf_df['fmt_GT'].apply(classifyZygosity)\n","\n","    #we still need to match REF/ALT\n","    config = tiledb.Config()\n","    # 11.53 MB buffer size, is this the culprit?\n","    config[\"py.init_buffer_bytes\"] = 1024**2 * 11\n","    ctx = tiledb.Ctx(config=config)\n","\n","    if selected_variants_uri is not None:\n","        with tiledb.open(selected_variants_uri,ctx=ctx) as selected_variants:\n","            sv_df_res = selected_variants.df[:]\n","            #clinvar pickle has these columns\n","            if variant_selection == 'CLINVARFILTERED':\n","                if(set(['contig','pos_start'])).issubset(sv_df_res.columns):\n","                    sv_df_res = sv_df_res.rename(columns={'contig':'CHROM','pos_start':'POS','GENEINFO':'GENE','pos_end':'clin_end'})\n","            elif variant_selection == 'ANNOTATED_VARIANT_DB':\n","                #select these columns: CHROM POS GENE clin_end CLINVAR_VARIANT_ID\n","                sv_df_res = sv_df_res[['CHROM','POS', 'REF','ALT','GENE','CLINVAR_ID']]\n","            \n","            assert(set(['CHROM','POS','REF','ALT']).issubset(sv_df_res.columns))\n","            assert(set(['contig','pos_start','ref','alt']).issubset(vcf_df.columns))\n","\n","            if subtract_start_pos_by_one_for_deletions:\n","                sv_df_res['ref_len'] = sv_df_res['REF'].str.len()\n","                sv_df_res['alt_len'] = sv_df_res['ALT'].str.len()\n","                sv_df_res.rename(columns={'POS':'POS_UNADJUSTED'},inplace=True)\n","                def subtract_start_pos(row):\n","                    if row['ref_len'] > row['alt_len']:\n","                        return row['POS_UNADJUSTED'] - 1\n","                    else:\n","                        return row['POS_UNADJUSTED']\n","\n","                # Apply the lambda function to the DataFrame\n","                sv_df_res['POS'] = sv_df_res.apply(subtract_start_pos, axis=1)\n","\n","\n","            if len(sv_df_res) == 0:\n","                return emptyPaTable\n","            else:\n","                print(f\"{len(sv_df_res)} results in the selected variants\")\n","\n","            sv_df_res['POS']=sv_df_res['POS'].astype(int)\n","            vcf_df['pos_start']=vcf_df['pos_start'].astype(int)\n","            vcf_df = vcf_df.merge(sv_df_res,left_on=['contig','pos_start','ref','alt'],right_on=['CHROM','POS','REF','ALT'],how=\"inner\")\n","            \n","            if len(vcf_df) == 0:\n","                print(\"no selected variants in the vcf\")\n","                return emptyPaTable\n","            else:\n","                print(f\"{len(vcf_df)} variant-sample tuples in the vcf after merge with selected variants\")\n","            \n","            hets = vcf_df[vcf_df['zygosity'] == 'HET']\n","\n","            \n","            if len(hets) > 0:\n","                grouped_hets = hets.groupby(['sample_name','GENE']).size().reset_index(name='het_count')\n","                compound_hets = grouped_hets[grouped_hets['het_count'] > 1].copy()\n","                if len(compound_hets) > 0:\n","                    #avoid the A value is trying to be set on a copy of a slice from a DataFrame error\n","                    compound_hets.loc[:,'compound_event']='CMPD_HET'\n","                    compound_hets.loc[:,'zygosity']='HET' #for the join\n","                    print(f\"{len(compound_hets)} compound hets found\")\n","                    vcf_df = vcf_df.merge(compound_hets,on=['sample_name','zygosity','GENE'],how=\"left\")\n","                else:\n","                    #columns should match up\n","                    vcf_df.loc[:,'het_count'] = 0\n","                    vcf_df.loc[:,'compound_event']=''\n","            else:\n","                vcf_df.loc[:,'het_count'] = 0\n","                vcf_df.loc[:,'compound_event']=''\n","            \n","            vcf_df['het_count'] = vcf_df['het_count'].fillna(0)\n","            vcf_df['het_count'] = vcf_df['het_count'].astype('int64')\n","            \n","    \n","    if len(vcf_df) == 0:\n","        return emptyPaTable\n","    else:\n","        print(f\"{len(vcf_df)} variant-sample tuples in vcf\")\n","    \n","    arrow_table = pa.Table.from_pandas(vcf_df)\n","    assert(isinstance(arrow_table,pyarrow.lib.Table))\n","    return arrow_table\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def combine_results(df_list, *args, **kwargs):\n","    import pyarrow\n","\n","    print(f\"Input list contains {len(df_list)} items\")\n","    results = []\n","    for x in df_list:\n","        if x is None:\n","            continue\n","        if not isinstance(x,pyarrow.lib.Table):\n","            #this can happen when pyarrow gets emotionally overwhelmed\n","            print(\"Not returing a pyarrow table, instead its a {type(x)}\")\n","            continue\n","        if x.num_rows>0:\n","            results+=[x]\n","        else:\n","            continue\n","    print('There are ', len(results), 'non-empty results')\n","    if len(results) > 1:\n","        table = pyarrow.concat_tables(results)\n","    elif len(results) == 1:\n","        table = results[0]\n","    else:\n","        table = None\n","    return table"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import tiledb\n","import numpy as np\n","\n","\n","dtypes_clinvar = {'internal_id':np.int32, 'contig':str, 'pos_start':np.int32, \n","       'CLINVAR_ID':str, 'REF':str, 'ALT':str, 'ALLELEID':str,\n","       'CLNDISDB':str, 'CLNDN':str, 'CLNHGVS':str, 'CLNREVSTAT':str, 'CLNSIG':str, 'CLNVC':str,\n","       'CLNVCSO':str, 'GENEINFO':str, 'ORIGIN':str, 'MC':str, 'pos':str, 'pos_end':str, 'pos_37':str,'pos_start_37':object,'pos_end_37':object,\n","       'NC':str, 'G_DOT':str,\n","       'NM':str, 'C_DOT':str, 'NP':str, 'P_DOT':str,'sub_classification':str}\n","\n","#Rady.list,inheritance.list,ID,CHROM,POS,REF,ALT,CLINVAR_VARIANT_ID,CLINVAR_ALLELE_ID,CLINVAR_DISEASE,CLINVAR_REVIEW_STATUS,CLINVAR_INTERP,GENE,CLINVAR_CONFLICT_CLASS,CLINVAR_INTERP_SIMPLE,MM_INTERP,MM_DISEASE,MM_VARIANT_ID,MM_INTERP_SIMPLE,JOINT_INTERP_SIMPLE,internal_id,CLNHGVS,CLNSIG,CLNVC,pos_38,pos_37,NC,G_DOT,NM,C_DOT,NP,P_DOT,sub_classification,Fabric_chr,Fabric_b37_pos,ACE.gene,ACE.consequence,ACE.classification,ACMG.rules.matched,Variant..UKB_EURpe.MCPS,Ref.Allele.MCPS,Alt.Allele.MCPS,CADD_PHRED.MCPS,Function.MCPS,Gene.Name.MCPS,Major.Hom.Ctrl.MCPS,Het.Ctrl.MCPS,Minor.Hom.Ctrl.MCPS,Minor.Hom.Ctrl.Freq.MCPS,Het.Ctrl.Freq.MCPS,Ctrl.Maf.MCPS,Ctrl.HWE_P.MCPS,Gene.ID,Transcript.Stable.Id,Has.CCDS.Transcript,Transcript.codon.change,Transcript.AA.change,.MCPSon.Rank,Gene.Transcript,LOF.Number.Of.Transcripts.In.Gene.Affected,LOF.Percentage.Of.Transcripts.Affected,OMIM_Number,OMIM_Pheno.UKB_EURpe,ClinVar.Disease,ClinVar.Clinical.Significance,ClinVar.PMID,MTR,MTR.FDR,MTR.Centile,REVEL,MGI_Pheno.UKB_EURpes,MGI_Essential,med_QUAL,med_PercAltRead,med_READS_REF,med_READS_ALT,med_GQ,med_FS,med_MQ,med_QD,med_MQRankSum,med_ReadPosRankSum,med_coverage,med_PL,med_DPF,med_R2_5P_bias,med_AF,med_SOR,med_FractionInformativeReads,Variant..UKB_EURpe.UKB_EUR,CADD_PHRED.UKB_EUR,Is.Minor.Ref,Function.UKB_EUR,Major.Hom.Ctrl.UKB_EUR,Het.Ctrl.UKB_EUR,Minor.Hom.Ctrl.UKB_EUR,Minor.Hom.Ctrl.Freq.UKB_EUR,Het.Ctrl.Freq.UKB_EUR,Missing.Ctrl,QC.Fail.Ctrl,Ctrl.Maf.UKB_EUR,Case.HWE_P,Ctrl.HWE_P.UKB_EUR\n","fields_genomenon = ['Rady.list', 'inheritance.list', 'ID', 'CHROM', 'POS', 'REF', 'ALT', 'CLINVAR_VARIANT_ID', 'CLINVAR_ALLELE_ID', 'CLINVAR_DISEASE', 'CLINVAR_REVIEW_STATUS', 'CLINVAR_INTERP', 'GENE', 'CLINVAR_CONFLICT_CLASS', 'CLINVAR_INTERP_SIMPLE', 'MM_INTERP', 'MM_DISEASE', 'MM_VARIANT_ID', 'MM_INTERP_SIMPLE', 'JOINT_INTERP_SIMPLE', 'internal_id', 'CLNHGVS', 'CLNSIG', 'CLNVC', 'pos_38', 'pos_37', 'NC', 'G_DOT', 'NM', 'C_DOT', 'NP', 'P_DOT', 'sub_classification', 'Fabric_chr', 'Fabric_b37_pos', 'ACE.gene', 'ACE.consequence', 'ACE.classification', 'ACMG.rules.matched', 'Variant..UKB_EURpe.MCPS', 'Ref.Allele.MCPS', 'Alt.Allele.MCPS', 'CADD_PHRED.MCPS', 'Function.MCPS', 'Gene.Name.MCPS', 'Major.Hom.Ctrl.MCPS', 'Het.Ctrl.MCPS', 'Minor.Hom.Ctrl.MCPS', 'Minor.Hom.Ctrl.Freq.MCPS', 'Het.Ctrl.Freq.MCPS', 'Ctrl.Maf.MCPS', 'Ctrl.HWE_P.MCPS', 'Gene.ID', 'Transcript.Stable.Id', 'Has.CCDS.Transcript', 'Transcript.codon.change', 'Transcript.AA.change', '.MCPSon.Rank', 'Gene.Transcript', 'LOF.Number.Of.Transcripts.In.Gene.Affected', 'LOF.Percentage.Of.Transcripts.Affected', 'OMIM_Number', 'OMIM_Pheno.UKB_EURpe', 'ClinVar.Disease', 'ClinVar.Clinical.Significance', 'ClinVar.PMID', 'MTR', 'MTR.FDR', 'MTR.Centile', 'REVEL', 'MGI_Pheno.UKB_EURpes', 'MGI_Essential', 'med_QUAL', 'med_PercAltRead', 'med_READS_REF', 'med_READS_ALT', 'med_GQ', 'med_FS', 'med_MQ', 'med_QD', 'med_MQRankSum', 'med_ReadPosRankSum', 'med_coverage', 'med_PL', 'med_DPF', 'med_R2_5P_bias', 'med_AF', 'med_SOR', 'med_FractionInformativeReads', 'Variant..UKB_EURpe.UKB_EUR', 'CADD_PHRED.UKB_EUR', 'Is.Minor.Ref', 'Function.UKB_EUR', 'Major.Hom.Ctrl.UKB_EUR', 'Het.Ctrl.UKB_EUR', 'Minor.Hom.Ctrl.UKB_EUR', 'Minor.Hom.Ctrl.Freq.UKB_EUR', 'Het.Ctrl.Freq.UKB_EUR', 'Missing.Ctrl', 'QC.Fail.Ctrl', 'Ctrl.Maf.UKB_EUR', 'Case.HWE_P', 'Ctrl.HWE_P.UKB_EUR']\n","dtypes_genomenon = {field: str for field in fields_genomenon}\n","dtypes_genomenon['POS'] = int\n","\n","\n","\n","dtypes = dtypes_genomenon\n","\n","if VARIANT_FILE_URI[VARIANT_SELECTION].endswith('.gz'):\n","       variant_df_full = pd.read_csv(VARIANT_FILE_URI[VARIANT_SELECTION], encoding='unicode_escape',dtype=dtypes,compression='gzip',low_memory=False)\n","else:\n","       variant_df_full = pd.read_csv(VARIANT_FILE_URI[VARIANT_SELECTION], encoding='unicode_escape',dtype=dtypes,low_memory=False)\n","\n","if VARIANT_SELECTION == 'GENOMENON' or VARIANT_SELECTION == 'ANNOTATED_VARIANT_DB':\n","       #rename CLINVAR_VARIANT_ID to CLINVAR_ID\n","       variant_df_full = variant_df_full.rename(columns={'CLINVAR_VARIANT_ID':'CLINVAR_ID'})\n","\n","variant_df = variant_df_full[['CHROM','POS','REF','ALT','GENE','CLINVAR_ID']]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["variant_df_full.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["metric = metric.append({'name':'variants_of_interest','description':'variants in selection list','stage':'initial','unit':'chr-pos-ref-alt','value':variant_df.shape[0]},ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#assert all CHROM starts with chr\n","assert(variant_df['CHROM'].str.startswith('chr').all())\n","\n","#assert all pos_start is int\n","variant_df.loc[:, 'POS'] = variant_df['POS'].astype(int)\n","assert(variant_df['POS'].dtype == int)\n","\n","#sort by CHR and POS\n","variant_df.sort_values(by=['CHROM','POS'],inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["variant_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["variant_df=variant_df.replace(np.nan,'',regex=True) \n","#show all the types in this dataframe\n","\n","#which are mixed dtypes?\n","for col in variant_df.columns:\n","    weird = (variant_df[[col]].applymap(type) != variant_df[[col]].iloc[0].apply(type)).any(axis=1)\n","    if len (variant_df[weird]) > 0:\n","        #set the column to the correct type\n","        variant_df[col] = variant_df[col].astype(str)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["variant_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(variant_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["blocklist=pd.read_csv(BLOCKLIST_FILE)\n","#rename blocklist columns to CHROM, POS, REF, ALT\n","blocklist = blocklist.rename(columns={'pos_start':'POS','ref':'REF','alt':'ALT'})\n","#drop the columns we don't need\n","blocklist = blocklist[['CHROM','POS','REF','ALT']]\n","if USE_BLOCKLIST:\n","       #use CHROM,POS,REF,ALT\n","       blocked_entries = variant_df.merge(blocklist,how='inner',on=['CHROM','POS','REF','ALT'])\n","\n","       block_all = variant_df.merge(blocked_entries.drop_duplicates(), on=['CHROM','POS','REF','ALT','GENE','CLINVAR_ID'], \n","                   how='left', indicator=True)      \n","       #remove the entries that are in the blocked_entries\n","       variant_df = block_all[block_all['_merge'] == 'left_only']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(variant_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["variant_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["metric = metric.append({'name':'variants_of_interest','description':'variants in selection list','stage':'after blocklist USE_BLOCKLIST:{}'.format(USE_BLOCKLIST),'unit':'chr-pos-ref-alt','value':variant_df.shape[0]},ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","if tiledb.array_exists(VARIANT_URI):\n","       print(\"Array already exists, skipping ingest\")\n","else:\n","       tiledb.from_pandas(uri=VARIANT_URI,dataframe=variant_df,mode=\"ingest\",row_start_idx=0,sparse=False)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["These are to be blocked or masked out of the variant dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(variant_df)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Beware there are some overlapping genes and variants assigned willy-nilly"]},{"cell_type":"raw","metadata":{"trusted":true},"source":["variant_df[variant_df['POS']==17388025]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def regions_snps_of_interest_ranges_by_gene_name(\n","    gene_name_list: List[str], selected_variants_uri: str\n",") -> List[List[str]]:\n","    if gene_name_list is None:\n","        with tiledb.open(selected_variants_uri) as A:\n","            sv_ds = A.query(return_arrow=False).df[:]\n","    else:\n","        with tiledb.open(selected_variants_uri) as A:\n","            sv_ds = A.query(return_arrow=False, cond=f\"GENE in {gene_name_list}\").df[:]\n","    #get rid of weird artefacts\n","    sv_df_res_filt = sv_ds[sv_ds['CHROM'].str.startswith('chr')]\n","    if gene_name_list:\n","        assert(sorted(list(set(sv_df_res_filt['GENE']))) == sorted(gene_name_list))\n","    gene_groups = sv_df_res_filt.sort_values(by=[\"CHROM\",\"POS\"]).groupby('GENE')\n","    regions = [[f'{row[\"CHROM\"]}:{row[\"POS\"]}-{row[\"POS\"]}' for _, row in gene.iterrows()] for _, gene in gene_groups]\n","    #this table has some entries with different allele changes same position\n","    for regionidx in range(len(regions)):\n","        regions[regionidx] = list(set(regions[regionidx]))\n","    return regions\n","\n","def regions_snps_of_interest_ranges_by_refseq_name(\n","    gene_name_list: List[str], selected_variants_uri: str\n",") -> List[List[str]]:\n","    if gene_name_list is None:\n","        with tiledb.open(selected_variants_uri) as A:\n","            sv_ds = A.query(return_arrow=False).df[:]\n","    else:\n","        with tiledb.open(selected_variants_uri) as A:\n","            sv_ds = A.query(return_arrow=False, cond=f\"NM in {gene_name_list}\").df[:]\n","    #get rid of weird artefacts caused by null fill values\n","    sv_df_res_filt = sv_ds[sv_ds['contig'].str.startswith('chr')]\n","    #sv_df_res_filt = sv_df_res_filt[~sv_df_res_filt['contig'].str.startswith('chrX')]\n","    if gene_name_list:\n","        assert(sorted(list(set(sv_df_res_filt['NM']))) == sorted(gene_name_list))\n","    gene_groups = sv_df_res_filt.sort_values(by=[\"contig\",\"pos_start\"]).groupby('NM')\n","    regions = [[f'{row[\"contig\"]}:{row[\"pos_start\"]}-{row[\"pos_start\"]}' for _, row in gene.iterrows()] for _, gene in gene_groups]\n","    #this table has some entries with different allele changes same position\n","    for regionidx in range(len(regions)):\n","        regions[regionidx] = list(set(regions[regionidx]))\n","    return regions\n","\n","def wholerange(\n","        snps_of_interest_ranges: List[List[str]],\n","):\n","    allranges = []\n","    for snps_of_interest_range in snps_of_interest_ranges:\n","        chrom = snps_of_interest_range[0].split(':')[0]\n","        # Extract start and end positions from each range\n","        positions = [list(map(int, r.split(':')[1].split('-'))) for r in snps_of_interest_range]\n","\n","        # Find the minimum start position and maximum end position\n","        min_start = min(pos[0] for pos in positions)\n","        max_end = max(pos[1] for pos in positions)\n","\n","        # Construct the overall range string\n","        allranges += [[f'{chrom}:{min_start}-{max_end}']]\n","\n","    print(allranges)\n","    return allranges"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Here you can subset the gene list for testing"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["gene_name_list_all = sorted(list(set(variant_df['GENE'])))\n","\n","gene_name_list = gene_name_list_all\n","\n","#return a nested list of 1bp regions convering snps per gene from the list of SNPs of interest\n","if VARIANT_SELECTION == 'CLINVARXML' or VARIANT_SELECTION == 'CLINVARFILTERED':\n","    regions = regions_snps_of_interest_ranges_by_refseq_name(gene_name_list,VARIANT_URI)\n","else:\n","    regions = regions_snps_of_interest_ranges_by_gene_name(gene_name_list,VARIANT_URI)\n","\n","wholeranges = wholerange(regions)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["metric = metric.append({'name':'regions_of_interest','description':'regions in selection list by gene or refseq name','stage':'initial','unit':'region','value':len(regions)},ignore_index=True)\n","metric = metric.append({'name':'whole_ranges_of_interest','description':'whole ranges in selection list min-max','stage':'initial','unit':'whole_range','value':len(wholeranges)},ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#make sure this remains sorted by chromosome\n","regions = sorted(regions,key=lambda x: x[0].split(':')[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#save regions to gzipped file, comma separated, one list per line\n","import gzip\n","with gzip.open(\"regions.txt.gz\", 'wt') as f:\n","    for region in regions:\n","        f.write(','.join(region)+'\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["VARIANT_URI"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(f\"{len(regions)} genes\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(f\"{sum(len(gene) for gene in regions)} loci \")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let's use realtime task graph mode for this query"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["graph = tiledb.cloud.dag.DAG(\n","    max_workers=60,\n","    namespace=namespace,\n","    name = \"radyANNOTATED_VARIANT_DB\",\n","    mode=DAG_MODE\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Fetch and annotate variants on a region (gene) basis, with presumably all samples in each partition\n","\n","We just pass regionlist list elements each one all the desired snps in a gene, no need to partition those further so set region_partition_count to None\n","\n","Might take 6-10min"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["sample_partition_count = 60\n","region_partition_count = 1 #1 means pull all the loci at once\n","all_reads = []\n","for region_part_id in [0]:\n","    for sample_part_id in range(sample_partition_count):\n","        results = graph.submit(\n","            read_partition,\n","            uri = TILEDB_VCF_URI[VCF_SELECTION], \n","            regions_by_gene = regions,\n","            samples = None,\n","            selected_variants_uri = VARIANT_URI,\n","            region_partition_count = region_partition_count,\n","            region_partition_id=region_part_id,\n","            sample_partition_count=sample_partition_count,\n","            sample_partition_id=sample_part_id,\n","            drop_duplicates = True,\n","            subtract_start_pos_by_one_for_deletions = adjust_position_for_clinvar,\n","            result_format=\"arrow\",\n","            local_mode=False,\n","            name=f\"Rady samples realtime query (Region {region_part_id}, Sample {sample_part_id})\",\n","            resource_class = \"standard\",\n","            mode=DAG_MODE,\n","            variant_selection=VARIANT_SELECTION,\n","            filter_for_quality = FILTER_FOR_QUALITY,\n","        )\n","        all_reads.append(results)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["metric = metric.append({'name':'sample_partition_count','description':'technical metadata about task graph partition','stage':'initial','unit':'int','value':sample_partition_count},ignore_index=True)\n","metric = metric.append({'name':'region_partition_count','description':'technical metadata about task graph partition','stage':'initial','unit':'int','value':region_partition_count},ignore_index=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["metric = metric.append({'name':'regions_of_interest','description':'regions queried in dag','stage':'pre-query','unit':'int','value':len(regions)},ignore_index=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(regions)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let's not send a list of prespecified variants but just pull down all variants in the region of interest, get VEP'ed and then join using gdot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["regions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for region in regions:\n","    print(f\"{len(region)} snps\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Combine the results"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["combined_vcf_results = graph.submit(\n","    combine_results, \n","    all_reads, \n","    name=\"Combinesamples\", \n","    mode=DAG_MODE,\n","    local_mode=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["%matplotlib inline\n","#graph.visualize()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["graph.compute()\n"]},{"cell_type":"raw","metadata":{},"source":["gnomad_graph.compute()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This can take about 10 minutes"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dev_mode = False\n","# ... and wait for it to finish.\n","retry = True\n","try:\n","    graph.wait()\n","except tiledb.cloud.tiledb_cloud_error.TileDBCloudError as tdberror:\n","    # Allow retry, if not in dev mode\n","    if not dev_mode:\n","        logger.error(f\"retriable graph error\\n{tdberror}\")\n","        retry = True\n","    else:\n","        logger.error(f\"fatal graph error\\n{tdberror}\")\n","        logger.debug(dag.server_logs(graph))\n","\n","# Retry once\n","if retry:\n","    try:\n","        logger.info(\"retry graph\")\n","        graph.retry_all()\n","        graph.wait()\n","    except tiledb.cloud.tiledb_cloud_error.TileDBCloudError as tdberror:\n","        logger.error(f\"graph error during retry, exiting\\n{tdberror}\")\n","        logger.debug(dag.server_logs(graph))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#one more retry just in case\n","graph.retry_all()\n","graph.wait()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if combined_vcf_results.result() is not None:\n","    vcf_df = combined_vcf_results.result().to_pandas()\n","else:\n","    print(\"No results\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vcf_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["len(vcf_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["metric = metric.append({'name':'vcf_df','description':'result of the main query','stage':'post-query','unit':'chr-pos-ref-alt-sample no dedup','value':vcf_df.shape[0]},ignore_index=True)"]},{"cell_type":"raw","metadata":{},"source":["These sample names should also be -1 free"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vcf_df['sample_name'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vcf_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["unique_combinations_df = vcf_df[['contig', 'pos_start']].drop_duplicates()\n","distinct_count = unique_combinations_df.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["distinct_count"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["len(vcf_df[['sample_name','contig','pos_start','ref','alt']].drop_duplicates())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#assert none of the sample-GENE singletons are called cmpd_het\n","assert(len(vcf_df.groupby(['sample_name','GENE','compound_event']).size().reset_index(name='count').query('count == 1')) == 0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vcf_df.groupby(['sample_name','GENE','compound_event']).size().reset_index(name='count')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["vcf_df['sample_name'].nunique()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This should capture 29 ABCC8 sample-variant tuples regardless of other genes in the query"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if VARIANT_SELECTION=='KINGSMORE':\n","    ABCC8 = vcf_df[vcf_df['GENE']=='ABCC8']\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vcf_df.columns"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This is important. If we want to filter for probands, make sure the VCF columns match up with the metadata and set `how = inner`"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["vcf_df = pd.merge(\n","    vcf_df, filtered_metadata, on=[\"sample_name\"], how=\"inner\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["filtered_metadata.columns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vcf_df['sample_name'].nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["len(vcf_df[['sample_name','contig','pos_start','ref','alt']].drop_duplicates())"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Actually drop any duplicates"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vcf_df = vcf_df.drop_duplicates(subset=['sample_name','contig','pos_start','ref','alt'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["metric = metric.append({'name':'vcf_df','description':'result of the main query','stage':'post-de-depup','unit':'distinct chr-pos-ref-alt-sample','value':vcf_df.shape[0]},ignore_index=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","def convert_to_tuple(value):\n","    if isinstance(value, list):\n","        return tuple(value)\n","    elif isinstance(value, np.ndarray):\n","        return str(value)\n","    return value\n","\n","# Iterate over each column and convert lists to tuples\n","vcf_df_nt = vcf_df.applymap(convert_to_tuple)\n","vcf_df_nt_nd = vcf_df_nt.drop_duplicates(subset=['sample_name','contig','pos_start','ref','alt'])\n","len(vcf_df_nt_nd)\n","# Output the modified DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["vcf_df_nt_nd.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["len(vcf_df[['sample_name','contig','pos_start','ref','alt']].drop_duplicates())"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["VCFs do not know males have one X chromosome. So set that manually."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#indeed chrX looks like 0/1 for males\n","def set_hemizygous(row):\n","    if row['gender'] == 'Male' and row['contig'] == 'chrX':\n","        return 'HEMI'\n","    else:\n","        return row['zygosity']\n","\n","# Apply the custom function to the DataFrame\n","vcf_df['zygosity'] = vcf_df.apply(set_hemizygous, axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vcf_df['GENE'] = vcf_df['GENE'].apply(lambda x: x.split(':')[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(vcf_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def read_moi_genes():\n","    nbs_moi_df = pd.read_csv(NBS_POI_FILE, sep='\\t')\n","    nbs_moi_df = nbs_moi_df.rename(columns = {'MOI':'MOI','Gene':'GENE'})\n","    new_df = nbs_moi_df.drop_duplicates(subset=['GENE'])\n","    gene_poi_map = {}\n","    for gene, poi in zip(new_df['GENE'], new_df['MOI']):\n","        gene_poi_map[gene.upper()] = poi\n","    return gene_poi_map\n","\n","gene_poi_map = read_moi_genes()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["NBS_POI_FILE"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gene_poi_map"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["vcf_df['moi'] = vcf_df['GENE'].map(gene_poi_map)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vcf_df['moi']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def positive_genotype(row):\n","    if row['moi'] == 'Pattern unknown':\n","        return \"Unknown\"\n","    elif row['moi'] == 'AD':\n","        return \"Yes\"\n","    elif row['moi'] == 'AR':\n","        if row['zygosity'] == 'HOM_ALT' or row['compound_event'] == 'CMPD_HET':\n","            return \"Yes\"\n","        else:\n","            return \"No\"\n","    elif row[\"moi\"] == 'XR':\n","        if row['gender'] == \"Female\":\n","            if row['zygosity'] == 'HOM_ALT' or row['compound_event'] == 'CMPD_HET':\n","                return \"Yes\"\n","        else:\n","            if row['zygosity'] == 'HEMI':\n","                return 'Yes'\n","        return \"No\"\n","    elif row[\"moi\"] == 'XD':\n","        if row['gender'] == \"Male\":\n","            if row['zygosity'] == 'HEMI':\n","                return 'Yes'\n","        else:\n","            return \"Yes\"\n","\n","\n","vcf_df['positive_genotype'] = vcf_df.apply(positive_genotype, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["metric = metric.append({'name':'vcf_df','description':'vcf_df entries postive wrt moi','stage':'moi_positive','unit':'distinct chr-pos-ref-alt-sample','value':len(vcf_df[vcf_df['positive_genotype']=='Yes'])},ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["columns = [col for col in vcf_df.columns if col not in ['individual_id', 'sample_name']]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["validation_data = pd.DataFrame({\n","    \"genome_id\": [949581, 963614, 949581, 963614, 1033213, 1053847, 1033213, 1053847],\n","    \"contig\": [\"chr18\", \"chr18\",\"chr18\",\"chr18\",\"chr18\",\"chr18\",\"chr18\",\"chr18\"],\n","    \"pos_start\": [23568833, 23573531, 23539394, 23544402, 23536736, 23533472, 23539859, 23539402],\n","    \"ref\": [\"ACT\", \"GCA\", \"G\", \"G\", \"A\", \"A\", \"T\", \"G\"],\n","    \"alt\": [\"A\", \"G\", \"A\", \"T\", \"G\", \"C\", \"C\", \"A\"],\n","    \"gene_symbol\": [\"NPC1\", \"NPC1\", \"NPC1\", \"NPC1\", \"NPC1\", \"NPC1\", \"NPC1\", \"NPC1\"],\n","    \"gene_id\": [\"ENSG00000141458\"]*8,\n","    \"canonical_hgvs_c\": [\"c.451_452del\", \"c.99_100del\", \"c.2872C>T\", \"c.2072C>A\", \"c.3182T>C\", \"c.3637T>G\", \"c.2747A>G\", \"c.2864C>T\"],\n","    \"canonical_hgvs_p\": [\"p.Ser151PhefsTer18\", \"p.Ala34IlefsTer23\", \"p.Arg958Ter\", \"p.Pro691Gln\", \"p.Ile1061Thr\", \"p.Leu1213Val\", \"p.Asn916Ser\", \"p.Ser955Phe\"],\n","    \"transcrip_id\": [\"ENST00000269228\"]*8,\n","    \"variant_classification\": [\"P\", \"P\", \"P\", \"LP\", \"P\", \"VUS\", \"VUS\", \"VUS\"]\n","})\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","pd.set_option(\"display.max_columns\", 100)\n","pd.set_option(\"display.max_colwidth\",500)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","validation_data.merge(vcf_df,on=['genome_id','contig','pos_start','ref','alt'],how='inner')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Check against this gold standard set of 300 variants"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# load the first sheet of the Excel file into a data frame\n","goldset = pd.read_excel(\"../../data/restrospective_variants_truth_set_052423.xlsx\", sheet_name=0).rename(columns={\"start_hg38\":\"pos_start\"})\n","goldset['contig'] = 'chr' + goldset['chr'].astype(str)\n","goldset.drop(columns=['chr','sample_name','zygosity','clinvar_variant_id'], inplace=True)\n","goldset = goldset.astype({'pos_start':'int'})\n","goldset.drop_duplicates(subset=['contig','pos_start','ref','alt'], inplace=True)\n","goldset.sort_values(by=['pos_start']).head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["goldset.columns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["chromosome_order = ['chr'+str(i) for i in range(1, 23)] + ['chrX', 'chrY', 'chrM']\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["goldset['contig'] = pd.Categorical(goldset['contig'], categories=chromosome_order, ordered=True)\n","goldset['chromosome_order'] = goldset['contig'].map(lambda c: chromosome_order.index(c))\n","\n","goldmerge = goldset.merge(vcf_df, on=['contig', 'pos_start','ref','alt'], how='left').sort_values(['chromosome_order', 'pos_start'])[['contig','pos_start','ref','alt','gene_symbol','zygosity','sample_name','CLINVAR_ID']]\n","goldmerge.to_csv('../../results/goldmerge.csv', index=False)\n","goldmerge.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["goldcount = goldmerge.groupby(['contig','pos_start','ref','alt']).agg(['nunique']).reset_index(drop=False).reset_index()\n","goldcount.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["metric = metric.append({'name':'vcf_df','description':'goldset entries','stage':'goldset','unit':'distinct chr-pos-ref-alt-sample','value':sum(goldcount['sample_name']['nunique'])},ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hotsamples = int((goldcount['sample_name']>1).sum())\n","print(\"{} of these {} variants has at least one sample\".format(hotsamples,len(goldcount)))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Cast the complex types as strings to work with `from_pandas`"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["obj_cols = vcf_df.select_dtypes(include=['object']).columns\n","vcf_df[obj_cols] = vcf_df[obj_cols].astype(str)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(vcf_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vcf_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#merge using CHROM,POS,REF,ALT,GENE,CLINVAR_VARIANT_ID,Rady.list\n","vcf_df = vcf_df.merge(variant_df_full,on=['CHROM','POS','REF','ALT','GENE'],how='inner')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["metric = metric.append({'name':'vcf_df','description':'result of the main query','stage':'post-anno','unit':'distinct chr-pos-ref-alt-sample','value':vcf_df.shape[0]},ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["metric.to_csv('../../results/metrics.tsv',index=False,sep=\"\\t\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vcf_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#gzip this file\n","vcf_df.to_csv('../../results/bulk_moi.tsv.gz', index=False, sep='\\t', compression='gzip')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vcf_df.to_csv('../../results/bulk_moi.tsv.zip', index=False, sep='\\t', compression='zip')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Python program to find MD5 hash value of a file\n","import hashlib\n"," \n","def md5sum(filename):\n","    md5_hash = hashlib.md5()\n","    with open(filename,\"rb\") as f:\n","        # Read and update hash in chunks of 4K\n","        for byte_block in iter(lambda: f.read(4096),b\"\"):\n","            md5_hash.update(byte_block)\n","        return(md5_hash.hexdigest())"]},{"cell_type":"raw","metadata":{},"source":["resultcsvs = []\n","for f in range(5):\n","    print(f)\n","    resultcsvs += [pd.read_csv(f\"../../results/{f}_bulk_moi.tsv.gz\",sep=\"\\t\",compression=\"gzip\",low_memory=False)[['sample_name','contig','pos_start','ref','alt']].drop_duplicates()]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#save this info to a metadata file\n","run_metadata = {\n","    \"VARIANT_SELECTION\":VARIANT_SELECTION,\n","    \"VARIANT_FILE_URI\":VARIANT_FILE_URI[VARIANT_SELECTION],\n","    \"VCF_SELECTION\":VCF_SELECTION,\n","    \"TILEDB_VCF_URI\":TILEDB_VCF_URI[VCF_SELECTION],\n","    \"NBS_POI_FILE\":NBS_POI_FILE,\n","    \"BLOCKLIST_FILE\":BLOCKLIST_FILE,\n","    \"USE_BLOCKLIST\":USE_BLOCKLIST,\n","    \"SAMPLE_USE_FILE\":SAMPLE_USE_FILE,\n","    \"START_POS_ADJUSTMENT\":adjust_position_for_clinvar,\n","    \"FILTER_FOR_QUALITY\":FILTER_FOR_QUALITY,\n","    \"bulk_moi\":'../../results/bulk_moi.tsv.gz',\n","    \"goldmerge\":'../../results/goldmerge.csv'\n","}\n","#fetch the md5sum of each file in the run_metadata\n","run_metadata['VARIANT_FILE_MD5'] = md5sum(VARIANT_FILE_URI[VARIANT_SELECTION])\n","run_metadata['NBS_POI_FILE_MD5'] = md5sum(NBS_POI_FILE)\n","run_metadata['BLOCKLIST_FILE_MD5'] = md5sum(BLOCKLIST_FILE)\n","run_metadata['SAMPLE_USE_FILE_MD5'] = md5sum(SAMPLE_USE_FILE)\n","run_metadata['bulk_moi_MD5'] = md5sum('../../results/bulk_moi.tsv.gz')\n","run_metadata['goldmerge_MD5'] = md5sum('../../results/goldmerge.csv')\n","\n","#get the line counts of each file in the run_metadata\n","run_metadata['VARIANT_FILE_LINECOUNT'] = !wc -l {VARIANT_FILE_URI[VARIANT_SELECTION]} | cut -d' ' -f1\n","run_metadata['NBS_POI_FILE_LINECOUNT'] = !wc -l {NBS_POI_FILE} | cut -d' ' -f1\n","run_metadata['BLOCKLIST_FILE_LINECOUNT'] = !wc -l {BLOCKLIST_FILE} | cut -d' ' -f1\n","run_metadata['SAMPLE_USE_FILE_LINECOUNT'] = !wc -l {SAMPLE_USE_FILE} | cut -d' ' -f1\n","run_metadata['bulk_moi_LINECOUNT'] = !gunzip -c ../../results/bulk_moi.tsv.gz | wc -l | cut -d' ' -f1\n","run_metadata['goldmerge_LINECOUNT'] = !wc -l ../../results/goldmerge.csv | cut -d' ' -f1\n","\n","import json\n","json.dump(run_metadata, open('../../results/run_metadata.json', 'w'), indent=4)"]},{"cell_type":"raw","metadata":{"trusted":true},"source":["import tiledb\n","#turn BLOCK_LIST_2022 and BLOCK_LIST into strings\n","vcf_df['BLOCK_LIST_2022'] = vcf_df['BLOCK_LIST_2022'].astype(str)\n","vcf_df['BLOCK_LIST'] = vcf_df['BLOCK_LIST'].astype(str)\n","tiledb.from_pandas(uri=REPORT_URI,dataframe=vcf_df,mode=\"ingest\",row_start_idx=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tiledb.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tiledbvcf\n","tiledbvcf.version"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import plotly.graph_objects as go"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["metric"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#just vcf_df\n","vcf_df_data = metric[metric['name']=='vcf_df'].copy().reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = go.Figure(data=[go.Sankey(\n","    node=dict(\n","        pad=15,\n","        thickness=20,\n","        line=dict(color=\"black\", width=0.5),\n","        label=vcf_df_data['name'] + '<br>' + vcf_df_data['stage']\n","    ),\n","    link=dict(\n","        source=[0, 1],\n","        target=[1, 4],\n","        value=vcf_df_data['value'][[0,1,4]]\n","    )\n",")])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.undefined"},"vscode":{"interpreter":{"hash":"fb84c64b30d2a9bcb873df7b8052c76e6c1aff1fc3386dc2a3ce98995537935a"}}},"nbformat":4,"nbformat_minor":4}
